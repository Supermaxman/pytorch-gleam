seed_everything: 0
model:
  class_path: pytorch_gleam.modeling.models.BertPreTrainLanguageModel
  init_args:
    learning_rate: 2e-5
    pre_model_name: digitalepidemiologylab/covid-twitter-bert-v2

trainer:
#  batch_size=1024
  max_epochs: 1
  accumulate_grad_batches: 4
  check_val_every_n_epoch: 1
  deterministic: true
  num_sanity_val_steps: 1
  checkpoint_callback: false
  callbacks:
    - class_path: pytorch_gleam.callbacks.FitCheckpointCallback
data:
  class_path: pytorch_gleam.data.datasets.BertPreDataModule
  init_args:
    batch_size: 8
    max_seq_len: 96
    tokenizer_name: digitalepidemiologylab/covid-twitter-bert-v2
    num_workers: 8
    train_path:
      - /shared/hltdir4/disk1/team/data/corpora/vax-lies/covid-19/stance-train.jsonl
    val_path:
      - /shared/hltdir4/disk1/team/data/corpora/vax-lies/covid-19/stance-dev.jsonl
